# Feature Roadmap: NeuroMuscleAI-MVP

目标：把当前演示级 Streamlit 前端扩展为对科研用户有实际帮助的工具，分阶段实现效果预判（置信度/不确定性）、在线微调、模型版本管理和批量评估。以下为建议的分阶段路线：

阶段 1 — 可见性与置信度（已实现/低风险）
- 在前端显示预测概率与不确定性（熵）。
- 添加温度缩放与置信度阈值配置。  
- UI 说明与人工复核建议提示。

阶段 2 — 批量评估与比较视图（中等风险）
- 支持上传一组样本（ZIP 或 CSV 列表），批量运行模型并生成混淆矩阵、AUC、F1 报告。
- 支持选择多个 checkpoint 进行并行评估与 Grad-CAM 对比。

阶段 3 — 人工-模型交互与在线微调（高风险，需异步任务）
- 在前端允许用户对单个样本标注并触发增量微调（限制 epochs，必须在后台任务队列中运行）。
- 引入任务队列（RQ 或 Celery），并在前端显示训练进度与结果（需更多 infra 支持）。

阶段 4 — 模型注册与回滚（组织级）
- 建立 `models/registry.json` 或使用轻量 DB 记录每个 checkpoint 的元数据（训练配置、数据版本、指标）。
- 前端支持切换模型、比较历史评估结果，并能回滚到指定版本。

阶段 5 — 自动化效果预判与告警（研究/生产）
- 统计模型对不同数据子群的表现（标签偏差、扫描设备偏差等），自动生成告警与改进建议。
- 集成数据漂移检测与自动提示需要人工复核的数据段。

实现注意事项
- 安全：所有在线训练/上传操作都要做配额与权限控制，避免滥用。  
- 可扩展性：阶段 3 之后需要考虑任务队列与持久化存储（S3 或网络共享）。
- 可重复性：记录训练/评估的 Git commit、配置与数据版本。

接下来我可以：
- 完成阶段 1 的前端改进（已完成），并把 `app.py` 做一次小幅重构以便后续扩展（例如把预测/评估逻辑抽成函数）。
- 基于阶段 2 生成更详细的实现任务清单和需要新增/修改的文件清单。

请告诉我你希望我继续哪个子任务（例如：把批量评估的 UI 加到前端；或把预测逻辑抽成可复用函数）。